{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O-SkdlPxvETZ"
   },
   "source": [
    "# Just In Time Compilation with JAX\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/02-jitting.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google/jax/blob/main/docs/jax-101/02-jitting.ipynb)\n",
    "\n",
    "*Authors: Rosalia Schneider & Vladimir Mikulik*\n",
    "\n",
    "In this section, we will further explore how JAX works, and how we can make it performant.\n",
    "We will discuss the `jax.jit()` transform, which will perform *Just In Time* (JIT) compilation\n",
    "of a JAX Python function so it can be executed efficiently in XLA.\n",
    "\n",
    "## How JAX transforms work\n",
    "\n",
    "In the previous section, we discussed that JAX allows us to transform Python functions. This is done by first converting the Python function into a simple intermediate language called jaxpr. The transformations then work on the jaxpr representation. \n",
    "\n",
    "We can show a representation of the jaxpr of a function by using `jax.make_jaxpr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P9Xj77Wx3Z2P",
    "outputId": "5a0597eb-86c9-4762-ce10-2811debbc732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = log a\n",
      "    c\u001b[35m:f32[]\u001b[39m = log 2.0\n",
      "    d\u001b[35m:f32[]\u001b[39m = div b c\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(d,) }\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "global_list = []\n",
    "\n",
    "def log2(x):\n",
    "  global_list.append(x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2)(3.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jiDsT7y0RwIp"
   },
   "source": [
    "The [Understanding Jaxprs](https://jax.readthedocs.io/en/latest/jaxpr.html) section of the documentation provides more information on the meaning of the above output.\n",
    "\n",
    "Importantly, note how the jaxpr does not capture the side-effect of the function: there is nothing in it corresponding to `global_list.append(x)`. This is a feature, not a bug: JAX is designed to understand side-effect-free (a.k.a. functionally pure) code. If *pure function* and *side-effect* are unfamiliar terms, this is explained in a little more detail in [ðŸ”ª JAX - The Sharp Bits ðŸ”ª: Pure Functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions).\n",
    "\n",
    "Of course, impure functions can still be written and even run, but JAX gives no guarantees about their behaviour once converted to jaxpr. However, as a rule of thumb, you can expect (but shouldn't rely on) the side-effects of a JAX-transformed function to run once (during the first call), and never again. This is because of the way that JAX generates jaxpr, using a process called 'tracing'.\n",
    "\n",
    "When tracing, JAX wraps each argument by a *tracer* object. These tracers then record all JAX operations performed on them during the function call (which happens in regular Python). Then, JAX uses the tracer records to reconstruct the entire function. The output of that reconstruction is the jaxpr. Since the tracers do not record the Python side-effects, they do not appear in the jaxpr. However, the side-effects still happen during the trace itself.\n",
    "\n",
    "Note: the Python `print()` function is not pure: the text output is a side-effect of the function. Therefore, any `print()` calls will only happen during tracing, and will not appear in the jaxpr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JxV2p7e2RawC",
    "outputId": "9dfe8a56-e553-4640-a04e-5405aea7832d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printed x: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n",
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = log a\n",
      "    c\u001b[35m:f32[]\u001b[39m = log 2.0\n",
      "    d\u001b[35m:f32[]\u001b[39m = div b c\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(d,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_with_print(x):\n",
    "  print(\"printed x:\", x)\n",
    "  ln_x = jnp.log(x)\n",
    "  ln_2 = jnp.log(2.0)\n",
    "  return ln_x / ln_2\n",
    "\n",
    "print(jax.make_jaxpr(log2_with_print)(3.))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f6W_YYwRRwGp"
   },
   "source": [
    "See how the printed `x` is a `Traced` object? That's the JAX internals at work.\n",
    "\n",
    "The fact that the Python code runs at least once is strictly an implementation detail, and so shouldn't be relied upon. However, it's useful to understand as you can use it when debugging to print out intermediate values of a computation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PgVqi6NlRdWZ"
   },
   "source": [
    "A key thing to understand is that jaxpr captures the function as executed on the parameters given to it. For example, if we have a conditional, jaxpr will only know about the branch we take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hn0CuphEZKZm",
    "outputId": "99dae727-d2be-4577-831c-e1e14af5890a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:i32[3]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(a,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_if_rank_2(x):\n",
    "  if x.ndim == 2:\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2.0)\n",
    "    return ln_x / ln_2\n",
    "  else:\n",
    "    return x\n",
    "\n",
    "print(jax.make_jaxpr(log2_if_rank_2)(jax.numpy.array([1, 2, 3])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp3WhqaqvHyD"
   },
   "source": [
    "## JIT compiling a function\n",
    "\n",
    "As explained before, JAX enables operations to execute on CPU/GPU/TPU using the same code.\n",
    "Let's look at an example of computing a *Scaled Exponential Linear Unit*\n",
    "([SELU](https://proceedings.neurips.cc/paper/6698-self-normalizing-neural-networks.pdf)), an\n",
    "operation commonly used in deep learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JAXFYtlRvD6p",
    "outputId": "e94d7dc2-a9a1-4ac2-fd3f-152e3f6d141b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.2 Âµs Â± 815 ns per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(1000000)\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ecN5lEXe6ncy"
   },
   "source": [
    "The code above is sending one operation at a time to the accelerator. This limits the ability of the XLA compiler to optimize our functions.\n",
    "\n",
    "Naturally, what we want to do is give the XLA compiler as much code as possible, so it can fully optimize it. For this purpose, JAX provides the `jax.jit` transformation, which will JIT compile a JAX-compatible function. The example below shows how to use JIT to speed up the previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nJVEwPcH6bQX",
    "outputId": "289eb2f7-a5ce-4cec-f652-5c4e5b0b86cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5 Âµs Â± 501 ns per loop (mean Â± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jax.jit(selu)\n",
    "\n",
    "# Warm up\n",
    "selu_jit(x).block_until_ready()\n",
    "\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hMNKi1mYXQg5"
   },
   "source": [
    "Here's what just happened:\n",
    "\n",
    "1) We defined `selu_jit` as the compiled version of `selu`.\n",
    "\n",
    "2) We called `selu_jit` once on `x`. This is where JAX does its tracing -- it needs to have some inputs to wrap in tracers, after all. The jaxpr is then compiled using XLA into very efficient code optimized for your GPU or TPU. Finally, the compiled code is executed to satisfy the call. Subsequent calls to `selu_jit` will use the compiled code directly, skipping the python implementation entirely.\n",
    "\n",
    "(If we didn't include the warm-up call separately, everything would still work, but then the compilation time would be included in the benchmark. It would still be faster, because we run many loops in the benchmark, but it wouldn't be a fair comparison.)\n",
    "\n",
    "3) We timed the execution speed of the compiled version. (Note the use of `block_until_ready()`, which is required due to JAX's [Asynchronous execution](https://jax.readthedocs.io/en/latest/async_dispatch.html) model)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DRJ6R6-d9Q_U"
   },
   "source": [
    "## Why can't we just JIT everything?\n",
    "\n",
    "After going through the example above, you might be wondering whether we should simply apply `jax.jit` to every function. To understand why this is not the case, and when we should/shouldn't apply `jit`, let's first check some cases where JIT doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GO1Mwd_3_W6g",
    "outputId": "a6fcf6d1-7bd6-4bb7-99c3-2a5a827183e2",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function f at /tmp/ipykernel_109033/1879943193.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m x\n\u001b[1;32m      9\u001b[0m f_jit \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mjit(f)\n\u001b[0;32m---> 10\u001b[0m f_jit(\u001b[39m10\u001b[39;49m)  \u001b[39m# Should raise an error. \u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x):\n\u001b[0;32m----> 4\u001b[0m   \u001b[39mif\u001b[39;00m x \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m      6\u001b[0m   \u001b[39melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_cuda11_tf_2_12_py_3_10_jupyter/lib/python3.10/site-packages/jax/_src/core.py:1369\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> 1369\u001b[0m   \u001b[39mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function f at /tmp/ipykernel_109033/1879943193.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "# Condition on value of x.\n",
    "\n",
    "def f(x):\n",
    "  if x > 0:\n",
    "    return x\n",
    "  else:\n",
    "    return 2 * x\n",
    "\n",
    "f_jit = jax.jit(f)\n",
    "f_jit(10)  # Should raise an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LHlipkIMFUhi",
    "outputId": "54935882-a180-45c0-ad03-9dfb5e3baa97",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function g at /tmp/ipykernel_109033/2522951876.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m i\n\u001b[1;32m      9\u001b[0m g_jit \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mjit(g)\n\u001b[0;32m---> 10\u001b[0m g_jit(\u001b[39m10\u001b[39;49m, \u001b[39m20\u001b[39;49m)  \u001b[39m# Should raise an error. \u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mg\u001b[0;34m(x, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(x, n):\n\u001b[1;32m      4\u001b[0m   i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m   \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m n:\n\u001b[1;32m      6\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      7\u001b[0m   \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m i\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_cuda11_tf_2_12_py_3_10_jupyter/lib/python3.10/site-packages/jax/_src/core.py:1369\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> 1369\u001b[0m   \u001b[39mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nThe problem arose with the `bool` function. \nThe error occurred while tracing the function g at /tmp/ipykernel_109033/2522951876.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument n.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "# While loop conditioned on x and n.\n",
    "\n",
    "def g(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "g_jit = jax.jit(g)\n",
    "g_jit(10, 20)  # Should raise an error. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "isz2U_XX_wH2"
   },
   "source": [
    "The problem is that we tried to condition on the *value* of an input to the function being jitted. The reason we can't do this is related to the fact mentioned above that jaxpr depends on the actual values used to trace it. \n",
    "\n",
    "The more specific information about the values we use in the trace, the more we can use standard Python control flow to express ourselves. However, being too specific means we can't reuse the same traced function for other values. JAX solves this by tracing at different levels of abstraction for different purposes.\n",
    "\n",
    "For `jax.jit`, the default level is `ShapedArray` -- that is, each tracer has a concrete shape (which we're allowed to condition on), but no concrete value. This allows the compiled function to work on all possible inputs with the same shape -- the standard use case in machine learning. However, because the tracers have no concrete value, if we attempt to condition on one, we get the error above.\n",
    "\n",
    "In `jax.grad`, the constraints are more relaxed, so you can do more. If you compose several transformations, however, you must satisfy the constraints of the most strict one. So, if you `jit(grad(f))`, `f` mustn't condition on value. For more detail on the interaction between Python control flow and JAX, see [ðŸ”ª JAX - The Sharp Bits ðŸ”ª: Control Flow](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow).\n",
    "\n",
    "One way to deal with this problem is to rewrite the code to avoid conditionals on value. Another is to use special [control flow operators](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators) like `jax.lax.cond`. However, sometimes that is impossible. In that case, you can consider jitting only part of the function. For example, if the most computationally expensive part of the function is inside the loop, we can JIT just that inner part (though make sure to check the next section on caching to avoid shooting yourself in the foot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OeR8hF-NHAML",
    "outputId": "d47fd6b2-8bbd-4939-a794-0b80183d3179"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(30, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While loop conditioned on x and n with a jitted body.\n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i = loop_body(i)\n",
    "  return x + i\n",
    "\n",
    "g_inner_jitted(10, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5XUT2acoHBz-"
   },
   "source": [
    "If we really need to JIT a function that has a condition on the value of an input, we can tell JAX to help itself to a less abstract tracer for a particular input by specifying `static_argnums` or `static_argnames`. The cost of this is that the resulting jaxpr is less flexible, so JAX will have to re-compile the function for every new value of the specified static input. It is only a good strategy if the function is guaranteed to get limited different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2yQmQTDNAenY",
    "outputId": "c48f07b8-c3f9-4d2a-9dfd-663838a52511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "f_jit_correct = jax.jit(f, static_argnums=0)\n",
    "print(f_jit_correct(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R4SXUEu-M-u1",
    "outputId": "9e712e14-4e81-4744-dcf2-a10f470d9121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "g_jit_correct = jax.jit(g, static_argnames=['n'])\n",
    "print(g_jit_correct(10, 20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify such arguments when using `jit` as a decorator, a common pattern is to use python's `functools.partial`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2X5rR4jkIO",
    "outputId": "81-4744-dc2e4-4e10f470f2-a19e71d9121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['n'])\n",
    "def g_jit_decorated(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    i += 1\n",
    "  return x + i\n",
    "\n",
    "print(g_jit_decorated(10, 20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LczjIBt2X2Ms"
   },
   "source": [
    "## When to use JIT\n",
    "\n",
    "In many of the examples above, jitting is not worth it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uMOqsNnqYApD",
    "outputId": "2d6c5122-43ad-4257-e56b-e77c889131c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g jitted:\n",
      "21.9 Âµs Â± 121 ns per loop (mean Â± std. dev. of 7 runs, 100,000 loops each)\n",
      "g:\n",
      "309 ns Â± 5.92 ns per loop (mean Â± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"g jitted:\")\n",
    "%timeit g_jit_correct(10, 20).block_until_ready()\n",
    "\n",
    "print(\"g:\")\n",
    "%timeit g(10, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cZmGYq80YP0j"
   },
   "source": [
    "This is because `jax.jit` introduces some overhead itself. Therefore, it usually only saves time if the compiled function is complex and you will run it numerous times. Fortunately, this is common in machine learning, where we tend to compile a large, complicated model, then run it for millions of iterations.\n",
    "\n",
    "Generally, you want to jit the largest possible chunk of your computation; ideally, the entire update step. This gives the compiler maximum freedom to optimise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hJMjUlRcIzVS"
   },
   "source": [
    "## Caching\n",
    "\n",
    "It's important to understand the caching behaviour of `jax.jit`.\n",
    "\n",
    "Suppose I define `f = jax.jit(g)`. When I first invoke `f`, it will get compiled, and the resulting XLA code will get cached. Subsequent calls of `f` will reuse the cached code. This is how `jax.jit` makes up for the up-front cost of compilation.\n",
    "\n",
    "If I specify `static_argnums`, then the cached code will be used only for the same values of arguments labelled as static. If any of them change, recompilation occurs. If there are many values, then your program might spend more time compiling than it would have executing ops one-by-one.\n",
    "\n",
    "Avoid calling `jax.jit` inside loops. For most cases, JAX will be able to use the compiled, cached function in subsequent calls to `jax.jit`. However, because the cache relies on the hash of the function, it becomes problematic when equivalent functions are redefined. This will cause unnecessary compilation each time in the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6MDSXCfmSZVZ",
    "outputId": "a035d0b7-6a4d-4a9e-c6b4-7521970829fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jit called in a loop with partials:\n",
      "621 ms Â± 42.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "jit called in a loop with lambdas:\n",
      "655 ms Â± 63.5 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "jit called in a loop with caching:\n",
      "4.13 ms Â± 185 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unjitted_loop_body(prev_i):\n",
    "  return prev_i + 1\n",
    "\n",
    "def g_inner_jitted_partial(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this! each time the partial returns\n",
    "    # a function with different hash\n",
    "    i = jax.jit(partial(unjitted_loop_body))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_lambda(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # Don't do this!, lambda will also return\n",
    "    # a function with a different hash\n",
    "    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n",
    "  return x + i\n",
    "\n",
    "def g_inner_jitted_normal(x, n):\n",
    "  i = 0\n",
    "  while i < n:\n",
    "    # this is OK, since JAX can find the\n",
    "    # cached, compiled function\n",
    "    i = jax.jit(unjitted_loop_body)(i)\n",
    "  return x + i\n",
    "\n",
    "print(\"jit called in a loop with partials:\")\n",
    "%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n",
    "\n",
    "print(\"jit called in a loop with lambdas:\")\n",
    "%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\n",
    "\n",
    "print(\"jit called in a loop with caching:\")\n",
    "%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Jitting functions in JAX",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
